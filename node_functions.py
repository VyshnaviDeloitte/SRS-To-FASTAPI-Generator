# Code Generated by Sidekick is for learning and experimentation purposes only.

import os
import logging
from typing import Dict, List, Any
from project_state import ProjectState, GenerationResult # Import the state definition
import tools # Import your tool functions
import llm_calls # Import your LLM call functions

logger = logging.getLogger(__name__)

# --- Node Implementations ---
def run_tests_and_validate(state: ProjectState) -> ProjectState:
    pass

def load_srs(state: ProjectState) -> ProjectState:
    """Node to load and parse the SRS document."""
    print("\n--- Node: load_srs ---")
    srs_path = state['srs_path']
    if not os.path.exists(srs_path):
        raise FileNotFoundError(f"SRS file not found at: {srs_path}")

    srs_text = tools.parse_docx(srs_path)
    if srs_text is None:
        raise ValueError(f"Failed to parse SRS file: {srs_path}")

    # TODO: Implement logic to detect if a schema image is present/referenced
    srs_image_path = None # Placeholder

    print(f"SRS text loaded successfully (length: {len(srs_text)}).")
    state['srs_text'] = srs_text
    state['srs_image_path'] = srs_image_path
    state['error_log'] = [] # Initialize error log
    state['generated_files'] = [] # Initialize file tracking
    state['persistent_context'] = {} # Initialize context
    state['documentation_files'] = [] # Initialize doc files list
    return state

def analyze_srs(state: ProjectState) -> ProjectState:
    """Node to analyze SRS text using LLM."""
    print("\n--- Node: analyze_srs ---")
    srs_text = state.get('srs_text')
    srs_image_path = state.get('srs_image_path')

    if not srs_text:
        raise ValueError("SRS text not found in state.")

    extracted_reqs = llm_calls.analyze_srs_llm(srs_text, srs_image_path)

    if not extracted_reqs:
        state['error_log'].append("Failed to extract requirements using LLM.")
        # Decide how to handle failure - maybe raise error or allow graph to proceed cautiously
        raise ValueError("LLM analysis failed to return structured requirements.")

    print("Requirements extracted successfully.")
    # TODO: Add validation for extracted_reqs structure
    state['requirements'] = extracted_reqs
    # Store summary for later use (e.g., documentation)
    state['persistent_context']['requirements_summary'] = f"Extracted {len(extracted_reqs.get('endpoints', []))} endpoints and {len(extracted_reqs.get('database_schema', {}).get('tables', []))} tables."

    return state

def setup_project_structure(state: ProjectState) -> ProjectState:
    """Node to create the basic FastAPI project folder structure and files."""
    print("\n--- Node: setup_project_structure ---")
    project_root = state['project_root'] # Should be set before calling the graph
    if not project_root:
        raise ValueError("Project root path not set in initial state.")

    print(f"Setting up project structure at: {project_root}")
    tools.create_directory(project_root)

    # Define structure
    dirs_to_create = [
        "app/api/routes",
        "app/models",
        "app/services",
        "app/core", # For config, etc.
        "tests/routes",
        "tests/services",
        "alembic/versions",
    ]
    files_to_create = {
        "app/__init__.py": "",
        "app/api/__init__.py": "",
        "app/api/routes/__init__.py": "",
        "app/models/__init__.py": "",
        "app/services/__init__.py": "",
        "app/core/__init__.py": "",
        "app/main.py": "# Placeholder for main FastAPI app\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get('/')\ndef read_root():\n    return {'message': 'API is running!'}\n",
        "app/database.py": "# Placeholder for DB connection\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nSQLALCHEMY_DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://user:password@host/db')\n\nengine = create_engine(SQLALCHEMY_DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n",
        "app/core/config.py": "# Placeholder for settings\nfrom pydantic_settings import BaseSettings\n\nclass Settings(BaseSettings):\n    APP_NAME: str = 'FastAPI App'\n    DATABASE_URL: str\n    # Add other settings\n\n    class Config:\n        env_file = '.env'\n\nsettings = Settings()\n",
        "tests/__init__.py": "",
        "tests/routes/__init__.py": "",
        "tests/services/__init__.py": "",
        "requirements.txt": "fastapi\nuvicorn[standard]\nsqlalchemy\npsycopg2-binary\nalembic\npydantic[email]\npython-dotenv\npytest\nhttpx\n# Add other dependencies identified later\n",
        ".env.example": "DATABASE_URL=postgresql://user:password@localhost:5432/mydatabase\n# Add other secrets/configs\n",
        "README.md": "# Project Title (Generated)\n\nBasic project structure generated by AI.\n",
        "Dockerfile": "# Placeholder Dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\n# CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\nEXPOSE 80\n",
        ".gitignore": "__pycache__/\n*.pyc\n*.pyo\n*.pyd\n.Python\n.env\n.venv/\nvenv/\n*.env\n*.log\n*.pot\n*.py[cod]\n.DS_Store\n",
        "alembic.ini": """[alembic]
script_location = alembic
sqlalchemy.url = %(DB_URL)s

[loggers]
# Add logger configurations here
[handlers]
# Add handler configurations here
[formatters]
# Add formatter configurations here
""",  # Basic alembic config pointing to env var
        "alembic/env.py": """# Placeholder alembic env.py
import os
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
from dotenv import load_dotenv

load_dotenv()

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
# target_metadata = None # Replace with your Base.metadata
from app.database import Base # Assuming Base is defined here
from app.models import * # Import all models to register them
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.

def run_migrations_offline():
   # ... (standard alembic offline code) ...
   url = os.getenv('DATABASE_URL')
   context.configure(
       url=url, target_metadata=target_metadata, literal_binds=True, dialect_opts={"paramstyle": "named"}
   )
   with context.begin_transaction():
       context.run_migrations()

def run_migrations_online():
   # ... (standard alembic online code) ...
   configuration = config.get_section(config.config_ini_section)
   configuration['sqlalchemy.url'] = os.getenv('DATABASE_URL')
   connectable = engine_from_config(
       configuration,
       prefix="sqlalchemy.",
       poolclass=pool.NullPool,
   )
   with connectable.connect() as connection:
       context.configure(connection=connection, target_metadata=target_metadata)
       with context.begin_transaction():
           context.run_migrations()

if context.is_offline_mode():
   run_migrations_offline()
else:
   run_migrations_online()
""",
        "alembic/script.py.mako": """# Mako template for Alembic migrations
# Add your migration script template here
""", # Standard template
    }

    # Create directories
    for d in dirs_to_create:
        tools.create_directory(os.path.join(project_root, d))

    # Create initial files
    current_files = state.get('generated_files', [])

    for file_rel_path, content in files_to_create.items():
        file_abs_path = os.path.join(project_root, file_rel_path)
        if tools.write_file(file_abs_path, content):
            current_files.append(GenerationResult(file_path=file_abs_path, status='written', description=f'Initial {os.path.basename(file_rel_path)}'))
        else:
            state['error_log'].append(f"Failed to write initial file: {file_rel_path}")

    state['generated_files'] = current_files
    print("Project structure setup complete.")
    # Add structure info to context if needed for LLMs
    state['persistent_context']['project_structure_summary'] = f"Created standard FastAPI structure under {project_root} with app, tests, alembic."
    return state

def generate_unit_tests(state: ProjectState) -> ProjectState:
    """Node to generate unit tests using LLM (TDD)."""
    print("\n--- Node: generate_unit_tests ---")
    requirements = state.get('requirements')
    project_root = state['project_root']
    context = state['persistent_context']
    current_files = state.get('generated_files', [])

    if not requirements or not project_root:
        raise ValueError("Missing requirements or project_root in state.")

    # Logic to decide which tests to generate (e.g., one file per route module)
    # Example: Generate tests for LMS endpoints
    lms_endpoints = [e for e in requirements.get('endpoints', []) if e['path'].startswith('/api/lms')]
    if lms_endpoints:
        target_test_path = os.path.join(project_root, 'tests/routes/test_lms_routes.py')
        print(f"Generating tests for LMS routes at {target_test_path}")
        test_code = llm_calls.generate_tests_llm(
            requirements={"endpoints": lms_endpoints}, # Pass only relevant part
            file_path=target_test_path,
            context=context
        )
        if test_code and tools.write_file(target_test_path, test_code):
            current_files.append(GenerationResult(file_path=target_test_path, status='generated', description='LMS route tests'))
            # Update context if needed
            context['lms_tests_generated'] = True
        else:
            state['error_log'].append(f"Failed to generate or write LMS tests: {target_test_path}")

    # Example: Generate tests for PODs endpoints
    pods_endpoints = [e for e in requirements.get('endpoints', []) if e['path'].startswith('/api/pods')]
    if pods_endpoints:
        target_test_path = os.path.join(project_root, 'tests/routes/test_pods_routes.py')
        print(f"Generating tests for PODs routes at {target_test_path}")
        test_code = llm_calls.generate_tests_llm(
            requirements={"endpoints": pods_endpoints},
            file_path=target_test_path,
            context=context
        )
        if test_code and tools.write_file(target_test_path, test_code):
            current_files.append(GenerationResult(file_path=target_test_path, status='generated', description='PODs route tests'))
            context['pods_tests_generated'] = True
        else:
            state['error_log'].append(f"Failed to generate or write PODs tests: {target_test_path}")

    # TODO: Add generation for service tests, etc.

    state['generated_files'] = current_files
    state['persistent_context'] = context
    print("Unit test generation attempt complete.")
    return state

def generate_implementation_code(state: ProjectState) -> ProjectState:
    """Node to generate models, services, routes using LLM."""
    print("\n--- Node: generate_implementation_code ---")
    requirements = state.get('requirements')
    project_root = state['project_root']
    context = state['persistent_context']
    current_files = state.get('generated_files', [])

    if not requirements or not project_root:
        raise ValueError("Missing requirements or project_root in state.")

    # 1. Generate Models based on schema
    db_schema = requirements.get('database_schema')
    if db_schema and db_schema.get('tables'):
        # Potentially generate one file per model or group related models
        target_model_path = os.path.join(project_root, 'app/models/models.py') # Example: one file
        print(f"Generating models at {target_model_path}")
        model_code = llm_calls.generate_code_llm(
            requirements={"database_schema": db_schema},
            target_file_path=target_model_path,
            tests_code=None, # No direct tests for models usually
            context=context
        )
        if model_code and tools.write_file(target_model_path, model_code):
            current_files.append(GenerationResult(file_path=target_model_path, status='generated', description='Database models'))
            # Update context with model names, etc.
            context['models_generated'] = True # Simple flag
            context['model_file_path'] = target_model_path # Store path
            # A better context would parse model_code to extract class names
        else:
            state['error_log'].append(f"Failed to generate or write models: {target_model_path}")

    # 2. Generate Services based on business logic/endpoints
    # Example: LMS Service
    lms_logic = [b for b in requirements.get('business_logic', []) if 'leave' in b.lower()]
    lms_endpoints = [e for e in requirements.get('endpoints', []) if e['path'].startswith('/api/lms')]
    if lms_logic or lms_endpoints:
        target_service_path = os.path.join(project_root, 'app/services/lms_service.py')
        print(f"Generating LMS service at {target_service_path}")
        # Find corresponding test code if generated
        lms_test_path = os.path.join(project_root, 'tests/services/test_lms_service.py') # Assume this path
        lms_test_code = tools.read_file(lms_test_path) if context.get('lms_service_tests_generated') else None

        service_code = llm_calls.generate_code_llm(
            requirements={"business_logic": lms_logic, "related_endpoints": lms_endpoints},
            target_file_path=target_service_path,
            tests_code=lms_test_code,
            context=context # Pass context (e.g., knows about model names)
        )
        if service_code and tools.write_file(target_service_path, service_code):
            current_files.append(GenerationResult(file_path=target_service_path, status='generated', description='LMS service logic'))
            context['lms_service_generated'] = True
        else:
            state['error_log'].append(f"Failed to generate or write LMS service: {target_service_path}")

    # TODO: Generate other services (PODs, Auth) similarly...

    # 3. Generate Routes based on endpoints
    # Example: LMS Routes
    if lms_endpoints:
        target_route_path = os.path.join(project_root, 'app/api/routes/lms_routes.py')
        print(f"Generating LMS routes at {target_route_path}")
        lms_test_path = os.path.join(project_root, 'tests/routes/test_lms_routes.py')
        lms_test_code = tools.read_file(lms_test_path) if context.get('lms_tests_generated') else None

        route_code = llm_calls.generate_code_llm(
            requirements={"endpoints": lms_endpoints},
            target_file_path=target_route_path,
            tests_code=lms_test_code,
            context=context # Pass context (knows about services, models)
        )
        if route_code and tools.write_file(target_route_path, route_code):
            current_files.append(GenerationResult(file_path=target_route_path, status='generated', description='LMS API routes'))
            context['lms_routes_generated'] = True
        else:
            state['error_log'].append(f"Failed to generate or write LMS routes: {target_route_path}")

    # TODO: Generate other routes (PODs, Auth, Dashboard)...

    # 4. Update main.py to include routers
    # TODO: Implement logic to parse generated route files, find APIRouter instances,
    # and generate code in main.py to include them. This is complex.
    # Simplified: Assume routers are named predictably (e.g., 'router' in each file)
    main_py_path = os.path.join(project_root, 'app/main.py')
    main_py_content = tools.read_file(main_py_path) or ""
    # Add includes (basic example)
    includes = ""
    if context.get('lms_routes_generated'):
        includes += "from app.api.routes import lms_routes\n"
        includes += "app.include_router(lms_routes.router, prefix='/api/lms', tags=['LMS'])\n"
    if context.get('pods_routes_generated'): # Check if PODs routes were generated
        includes += "from app.api.routes import pods_routes\n"

    # Add other includes...

    # Basic way to add includes (prone to errors if placeholder changes)
    if includes and "# Add routers here" not in main_py_content:
        main_py_content = main_py_content.replace(
           "app = FastAPI()",
           f"app = FastAPI()\n\n# Add routers here\n{includes}"
        )
        if tools.write_file(main_py_path, main_py_content):
            print("Updated main.py with router includes.")
            current_files.append(GenerationResult(file_path=main_py_path, status='updated', description='Included API routers'))
        else:
            state['error_log'].append(f"Failed to update main.py: {main_py_path}")